Explain in detail:
● HDFS
● Hadoop cluster
● HDFS blocks

Ans 
HDFS:
 Hadoop Distributed File System (HDFS) 
– File system component of Hadoop
– Store metadata on a dedicated server NameNode
– Store application data on other servers DataNode
– TCP-based protocols
– Replication for reliability
– Multiply data transfer bandwidth for durability
Hadoop (http://hadoop.apache.org) was originally built by a Yahoo!
engineer named Doug Cutting and is now an open source project managed
by the Apache Software Foundation. It is made available under the Apache
License v2.0.
The Hadoop Distributed File System is a versatile, resilient, clustered approach to managing files in a 
big data environment. HDFS is not the final destination for files. Rather, it is a data service that offers
a unique set of capabilities needed when data volumes and velocity are high. Because the data is written once
and then read many times thereafter, rather than the constant read-writes of other file systems, HDFS is an 
excellent choice for supporting big data analysis. The service includes a “NameNode” and multiple “data nodes”
running on a commodity hardware cluster and provides the highest levels of performance when the entire cluster
is in the same physical rack in the data center. In essence, the NameNode keeps track of where data is physically stored.

HADOOP CLUSTER:
A Hadoop cluster is a special type of computational cluster designed specifically for 
storing and analyzing huge amounts of unstructured data in a distributed computing environment. 
Such clusters run Hadoop's open source distributed processing software on low-cost commodity computers. 
HDFS follows the master-slave architecture and it has the following elements.

Namenode
The namenode is the commodity hardware that contains the GNU/Linux operating system and the namenode software.
It is a software that can be run on commodity hardware. The system having the namenode acts as the master server
and it does the following tasks:
Manages the file system namespace.
Regulates client’s access to files.
It also executes file system operations such as renaming, closing, and opening files and directories.

Datanode
The datanode is a commodity hardware having the GNU/Linux operating system and datanode software.
For every node (Commodity hardware/System) in a cluster, there will be a datanode. These nodes manage 
the data storage of their system.
Datanodes perform read-write operations on the file systems, as per client request.
They also perform operations such as block creation, deletion, and replication according to the instructions
of the namenode.

HADOOP BLOCKS
Filesystem Blocks: A block is the smallest unit of data that can be stored or retrieved from the disk. Filesystems
deal with the data stored in blocks. Filesystem blocks are normally in few kilobytes of size. Blocks are transparent
to the user who is performing filesystem operations like read and write.

HDFS Block
Hadoop distributed file system also stores the data in terms of blocks. However the block size in HDFS is very large.
The default size of HDFS block is 64MB. The files are split into 64MB blocks and then stored into the hadoop filesystem.
The hadoop application is responsible for distributing the data blocks across multiple nodes. 

Advantages of HDFS Block

The benefits with HDFS block are: 
The blocks are of fixed size, so it is very easy to calculate the number of blocks that can be stored on a disk.
HDFS block concept simplifies the storage of the datanodes. The datanodes doesn’t need to concern about the blocks metadata 
data like file permissions etc. The namenode maintains the metadata of all the blocks.
If the size of the file is less than the HDFS block size, then the file does not occupy the complete block storage.
As the file is chunked into blocks, it is easy to store a file that is larger than the disk size as the data blocks are 
distributed and stored on multiple nodes in a hadoop cluster.
Blocks are easy to replicate between the datanodes and thus provide fault tolerance and high availability. Hadoop framework
replicates each block across multiple nodes (default replication factor is 3). In case of any node failure or block corruption,
the same block can be read from another node.

HDFS Blocks are Large in Size
The main reason for having the HDFS blocks in large size is to reduce the cost of seek time. In general, the seek time is 10ms
and disk transfer rate is 100MB/s. To make the seek time 1% of the disk transfer rate, the block size should be 100MB. The default
size HDFS block is 64MB. 
